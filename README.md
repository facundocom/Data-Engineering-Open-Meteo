# Data Engineering Pipeline: Open-Meteo a MinIO (Delta Lake)

Este proyecto implementa un pipeline de ingenier√≠a de datos (ETL) dise√±ado para extraer datos meteorol√≥gicos (pron√≥sticos e hist√≥ricos) de la API de **Open-Meteo** y almacenarlos en un **Object Storage (MinIO)** utilizando el formato **Delta Lake**.

El caso de uso actual est√° configurado para monitorear las condiciones clim√°ticas en **Cuesta del Viento, San Juan, Argentina**.

## üöÄ Caracter√≠sticas

* **Extracci√≥n de Datos:** Obtiene variables horarias como temperatura (2m), velocidad del viento (10m), r√°fagas de viento (10m) y direcci√≥n del viento (10m).
* **Gesti√≥n de API:** Implementa cach√© y reintentos autom√°ticos (retries) para optimizar las peticiones a Open-Meteo y manejar fallos de red.
* **Almacenamiento Eficiente:** Utiliza **Delta Lake** sobre S3/MinIO, lo que permite transacciones ACID y un manejo eficiente de versiones de datos.
* **Carga Incremental (Forecast):** Soporta `upserts` (merge) para actualizar los pron√≥sticos sin duplicar datos. Inserta nuevos registros o actualiza los existentes bas√°ndose en la fecha ("src.date = tgt.date").
* **Carga Hist√≥rica:** Permite la carga masiva de datos hist√≥ricos organizados por a√±o y mes (rango configurado por defecto: 2019-2024).

## üìÇ Estructura del Proyecto

El proyecto est√° modularizado para separar responsabilidades de extracci√≥n, configuraci√≥n y carga:

.
‚îú‚îÄ‚îÄ main.py                     # Punto de entrada. Orquesta la extracci√≥n y la carga de datos.
‚îú‚îÄ‚îÄ datapipeline/
‚îÇ   ‚îú‚îÄ‚îÄ gestorAPI.py            # Configuraci√≥n del cliente API Open-Meteo (params, cache, retry).
‚îÇ   ‚îú‚îÄ‚îÄ gestorEXT.py            # L√≥gica de extracci√≥n: realiza las peticiones y transforma a Pandas DataFrame.
‚îÇ   ‚îú‚îÄ‚îÄ gestorCARGA.py          # L√≥gica de carga: escribe DataFrames en MinIO usando Delta Lake.
‚îÇ   ‚îî‚îÄ‚îÄ MINIOconfig.py          # Configuraci√≥n de conexi√≥n al bucket S3/MinIO (credenciales y endpoints).
‚îî‚îÄ‚îÄ .gitignore                  # Archivos ignorados por git (ej. __pycache__, .vscode).

## üõ†Ô∏è Requisitos e Instalaci√≥n

Este proyecto requiere **Python 3.x**. Las principales librer√≠as utilizadas son:

* `openmeteo-requests`
* `requests-cache`
* `retry-requests`
* `deltalake`
* `pandas`
* `pyarrow`
* `apscheduler` (para la ejecuci√≥n programada)

### Instalaci√≥n de dependencias

Puedes instalar todas las librer√≠as necesarias ejecutando el siguiente comando:

pip install openmeteo-requests requests-cache retry-requests deltalake pandas pyarrow apscheduler

## ‚öôÔ∏è Configuraci√≥n

### 1. Configuraci√≥n de MinIO (datapipeline/MINIOconfig.py)
Actualmente, las credenciales est√°n definidas dentro del c√≥digo. 
**Importante:** Para un entorno productivo, se recomienda usar variables de entorno. Para pruebas locales, aseg√∫rate de que `AWS_ENDPOINT_URL`, `AWS_ACCESS_KEY_ID` y `AWS_SECRET_ACCESS_KEY` coincidan con tu instancia de MinIO.

### 2. Par√°metros de Ubicaci√≥n (main.py)
Por defecto, el script busca datos para **Cuesta del Viento**. Puedes modificar las coordenadas y la zona horaria en el bloque principal de `main.py`:

latitud_cuesta = -30.183
longitud_cuesta = -69.066
timezone_cuesta = "America/Argentina/San_Juan"

## ‚ñ∂Ô∏è Uso

Para ejecutar el pipeline manualmente, corre el script principal:

python main.py

### Flujo de Ejecuci√≥n en `main.py`:

1.  **Inicializaci√≥n:** Se configuran los gestores de API (`APIconfig`), extracci√≥n (`extraccion`), carga (`carga`) y almacenamiento (`configMINIO`).
2.  **Carga Incremental (Forecast):** * Llama a `extincremental`.
    * Intenta realizar un `upsert` (merge) con los datos del pron√≥stico del d√≠a.
    * Si la tabla Delta no existe (primera ejecuci√≥n), realiza una carga inicial (`carga_forecast_overw`).
3.  **Carga Hist√≥rica:** * Llama a `gc.carga_historical_overw`.
    * Itera sobre el rango de a√±os y meses definido para descargar y guardar el historial.

> **Automatizaci√≥n:** El c√≥digo incluye bloques comentados para usar `BlockingScheduler`, lo que permitir√≠a ejecutar el proceso autom√°ticamente todos los d√≠as a una hora espec√≠fica.

## üìä Datos Generados (Data Lake)

Los datos se guardan en el bucket especificado (`facundocoria-bucket`) siguiendo una estructura de **Bronze Layer**:

* **Forecast (Pron√≥stico):** Ruta: `datalake/bronze/forecast/YYYY/MM/DD`
* **Historical (Hist√≥rico):** Ruta: `datalake/bronze/historical/YYYY/MM`

Al utilizar **Delta Lake**, estos datos pueden ser consultados posteriormente utilizando motores como Spark, Trino, o nuevamente con Python (`deltalake` / `pandas`).
